{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#全局变量\n",
    "import torch\n",
    "\n",
    "# Data Information\n",
    "\n",
    "\n",
    "TRAINING_BATCH_SIZE = 64\n",
    "\n",
    "TESTING_BATCH_SIZE = 1024\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ENCODER_DIM = 512 #由图片的最后一个维度决定\n",
    "\n",
    "DECODER_DIM = 512\n",
    "\n",
    "ATTENTION_DIM = 256\n",
    "\n",
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "EMBED_DIM = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6922042deefd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#导入字典\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Using the fastest word splitting tools\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mSPACY_OBJ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"en_core_web_sm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "#导入字典\n",
    "import spacy\n",
    "\n",
    "#Using the fastest word splitting tools\n",
    "SPACY_OBJ = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Defining our dictionary for all the words contained in the provided captions\n",
    "class MyVocab:\n",
    "    \"\"\"\n",
    "        This class is responsible for constructing the dictionary which contains\n",
    "        all the words that appear over a certain frequency, which we will use to\n",
    "        tokenize any given sentence for our RNN model.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        #Pre restore the tokens mapping.\n",
    "        \"\"\"\n",
    "        These are severals pre-defined tokens to pre process the sentence(sequqnce).\n",
    "        <PAD>: Used to pad any given sentence to a uniform length, making it easier for\n",
    "        RNN model to handle.\n",
    "        <SOS>: Inserted at the start of each sentence.\n",
    "        <EOS>: Appended at the end of each sentence.\n",
    "        <UNK>: Mark the word that hasn't appeared in the captions in the training data.\n",
    "        \"\"\"\n",
    "        self.index_to_tokens = {0:\"<PAD>\",1:\"<SOS>\",2:\"<EOS>\",3:\"<UNK>\"}\n",
    "\n",
    "        #Inverse the above dictionary\n",
    "        self.tokens_to_index = {value:key for key,value in self.index_to_tokens.items()}\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        :return: int The number of the stored tokens\n",
    "        \"\"\"\n",
    "        return len(self.index_to_tokens)\n",
    "\n",
    "\n",
    "    def build_vocab(self,sentence_list,min_count=1,max_count=None,max_features=None):\n",
    "        \"\"\"\n",
    "        This function builds the dictionary for RNN model\n",
    "        :param sentence_list: An iterable containers that includes all the sentences\n",
    "        :param min_count: The minimum number of the time that a word should appear in all the sentences.\n",
    "        :param max_count: The maximum number of the time that a word should appear in all the sentences.\n",
    "        :param max_features: Number of words to keep(From the most frequent words).\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        #Create a dictionary for counting word frequency\n",
    "        self.frequency_counter = {}\n",
    "\n",
    "        #Create word_dict from several sentences\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenize(sentence):\n",
    "                self.frequency_counter[word] = self.frequency_counter.get(word,0)+1\n",
    "\n",
    "        #Filtering\n",
    "        if min_count is not None:\n",
    "            self.frequency_counter = {word:value for word,value in self.frequency_counter.items() if value >= min_count}\n",
    "\n",
    "        if max_count is not None:\n",
    "            self.frequency_counter = {word:value for word,value in self.frequency_counter.items() if value <= max_count}\n",
    "\n",
    "        if max_features is not None:\n",
    "            self.frequency_counter = dict(list(sorted(self.frequency_counter.items(),key=lambda x:x[-1],reverse=True))[:max_features])\n",
    "\n",
    "        #Creating words_to_index mapping\n",
    "        for word in self.frequency_counter:\n",
    "            self.tokens_to_index[word] = len(self.tokens_to_index)\n",
    "\n",
    "\n",
    "        #Creating index_to_words mapping\n",
    "        self.index_to_tokens = dict(zip(self.tokens_to_index.values(),self.tokens_to_index.keys()))\n",
    "\n",
    "\n",
    "\n",
    "    def sentence_to_index(self,sentence,max_len = 20):\n",
    "        \"\"\"\n",
    "        This function converts the sentence to word index and controls\n",
    "        the maximum length of the sentence. Meanwhile, it adds <SOS> and <EOS> tags\n",
    "        to the beginning and the ending of a given sentence.\n",
    "        :param sentence: string, A sentence in string.\n",
    "        :param max_len: int, performing sentence pruning\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        tokenized_sentence = self.tokenize(sentence)\n",
    "        if max_len is not None:\n",
    "            tokenized_sentence = tokenized_sentence[:max_len]\n",
    "\n",
    "        return [self.tokens_to_index.get(word,self.tokens_to_index[\"<UNK>\"]) for word in tokenized_sentence]\n",
    "\n",
    "\n",
    "    def index_to_sentence(self,indices):\n",
    "        \"\"\"\n",
    "        This function converts the index back to caption words, used for visualization.\n",
    "        :param indices: A list of the index of words, e.g. [2,3,6,9,10,...]\n",
    "        :return: A list of word corresponding to the indices, [\"today\",\"good\",\"date\",...]\n",
    "        \"\"\"\n",
    "        return [self.index_to_tokens.get(index) for index in indices]\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(content):\n",
    "        # Filter out the unwanted signs(Unlikely to be seen in a generated sentence.\n",
    "        tokens = [token.text.lower() for token in SPACY_OBJ.tokenizer(content)]\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-24bfaad35d0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCompose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mResize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mRandomCrop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision.transforms import Compose,ToTensor,Normalize,Resize,RandomCrop\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "#Doing Data Processing\n",
    "class Flickr8K(Dataset):\n",
    "    def __init__(self,root_path,captions_path,transform=None,train = True):\n",
    "        \"\"\"\n",
    "        This class create a self-defined dataset, which integrates image\n",
    "        preprocessing and captions preprocessing.\n",
    "\n",
    "        :param root_path: root file location for the data, for a more compatible transplant between group members\n",
    "        :param captions_path: relative file location of captions.txt, and processed for RNN NetWork\n",
    "        :param transform: Transform the image data to fit better in the CNN Network\n",
    "        \"\"\"\n",
    "\n",
    "        # Predefine the some basic attribute\n",
    "        self.root_path = root_path\n",
    "        self.pandas_dataframe = pd.read_csv(captions_path)\n",
    "        self.transform_img = transform\n",
    "\n",
    "\n",
    "        # Get the images and captions\n",
    "        self.imgs_list = self.pandas_dataframe[\"image\"]\n",
    "        self.captions_list = self.pandas_dataframe[\"caption\"]\n",
    "\n",
    "\n",
    "        # Getting the word_dictionary created from the input captions\n",
    "        self.word_dictionary = MyVocab()\n",
    "        self.word_dictionary.build_vocab(list(self.captions_list))\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        \"\"\"\n",
    "        Getting the single entry of the data, here in our project, we are going\n",
    "        to use img as input, and caption as target label. Reference from\n",
    "        torchvision.datasets.MNIST()\n",
    "\n",
    "        :param index: int.  The index of the entry.\n",
    "        :return: tuple. A single entry of the data. (img,caption)\n",
    "        img: tensor object ([channels,width,height])\n",
    "        caption_to_index: tensor object [3,2,4,5,1,2,....,4]\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the name of the image. 10...12.jpg\n",
    "        img_name = self.imgs_list[index]\n",
    "        # Get the path of the image.\n",
    "        img_path = os.path.join(self.root_path,img_name)\n",
    "        # Reference from MINST class, Implement the img_loader\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "        #Transform the image\n",
    "        if self.transform_img:\n",
    "            img = self.transform_img(img)\n",
    "\n",
    "\n",
    "        # Process the captions\n",
    "        caption = self.captions_list[index]\n",
    "\n",
    "        caption_to_index = [self.word_dictionary.tokens_to_index[\"<SOS>\"],\n",
    "                            *self.word_dictionary.sentence_to_index(caption),\n",
    "                            self.word_dictionary.tokens_to_index[\"<EOS>\"]]\n",
    "\n",
    "        return (img,torch.tensor(caption_to_index))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        :return: Number of the entries of the imgs_captions dataframe\n",
    "        \"\"\"\n",
    "        return len(self.pandas_dataframe)\n",
    "\n",
    "\n",
    "\n",
    "class ProcessCaption:\n",
    "    \"\"\"\n",
    "    This class tries to fix the ingrained problem in DataLoader\n",
    "    Class, where it will mistakenly recognize img_pixel as container_abcs.Sequence\n",
    "    data type, which will result in the recursive pruning of the\n",
    "    caption index list.\n",
    "    \"\"\"\n",
    "    def __init__(self,pad_idx,batch_first = False):\n",
    "        self.pad_idx = pad_idx\n",
    "        self.batch_fist = batch_first\n",
    "\n",
    "    def __call__(self,batch):\n",
    "        \"\"\"\n",
    "\n",
    "        :param batch: An entry of data\n",
    "        :return: tuple: (tensor,tensor)\n",
    "        img_pixel: tensor.Size([batch_size,channels,height,width])\n",
    "        caption: tensor.Size([batch_size,seq_len])\n",
    "        \"\"\"\n",
    "\n",
    "        # Adding an axis at the batch_size dimension\n",
    "        img_pixel = [item[0].unsqueeze(0) for item in batch]\n",
    "\n",
    "        # Concating the tensor at the batch_size dimension\n",
    "        img_pixel = torch.cat(img_pixel, dim=0)\n",
    "\n",
    "\n",
    "        caption = [item[1] for item in batch]\n",
    "        caption = torch.nn.utils.rnn.pad_sequence(caption, batch_first=self.batch_fist,\n",
    "                                                  padding_value=self.pad_idx)\n",
    "\n",
    "        return img_pixel, caption\n",
    "\n",
    "\n",
    "DATA_PATH = \"C:/Users/DELL/Desktop/NYU/NYU Class Materials/NYUSpring2021Courses/Machine Learning/Final Project/数据集\"\n",
    "\n",
    "# Used to transform the original Images\n",
    "pre_transform = Compose([Resize((224,224)),\n",
    "                         RandomCrop(224),\n",
    "                         ToTensor(),\n",
    "                         Normalize((0.485,0.456,0.406),(0.229,0.224,0.225))\n",
    "                         ])\n",
    "\n",
    "dataset = Flickr8K(root_path=DATA_PATH + \"/Images\",\n",
    "                       captions_path=DATA_PATH + \"/captions.txt\",\n",
    "                       transform=pre_transform,\n",
    "                       train = True\n",
    "                       )\n",
    "\n",
    "VOCAB_DIM = len(dataset.word_dictionary)\n",
    "\n",
    "PAD_IDX = dataset.word_dictionary.tokens_to_index[\"<PAD>\"]\n",
    "\n",
    "\n",
    "def get_data_loader(train=True):\n",
    "    \"\"\"\n",
    "    This function generate the dataset needed for either training set\n",
    "    or testing set\n",
    "    :param train:\n",
    "    :return: A dataloader object(iterable)\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        batch_size = TRAINING_BATCH_SIZE\n",
    "    else:\n",
    "        batch_size = TESTING_BATCH_SIZE\n",
    "\n",
    "    data_loader = DataLoader(dataset=dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=True,\n",
    "                             collate_fn=ProcessCaption(pad_idx=PAD_IDX,batch_first=True))\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "def show_image(img_pixel,label = None):\n",
    "    \"\"\"\n",
    "    This is an auxiliary function for\n",
    "    :param img_pixel: tensor object [channels,width,height]\n",
    "    \"\"\"\n",
    "\n",
    "    # Reshape the image for imshow => [width,height,channels]\n",
    "    img_pixel = img_pixel.numpy().transpose((1,2,0))\n",
    "    plt.imshow(img_pixel)\n",
    "    if label:\n",
    "        plt.title(label)\n",
    "    plt.pause(0.001)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class MyEncoderCNN(nn.Module):\n",
    "    def __init__(self,cnn_type=\"resnet\"):\n",
    "        \"\"\"\n",
    "        Here we are going to use the pretrained resnet50,vggnet19 model, which is capsulated in\n",
    "        torchvision.models. This model is trained on mindspore dataset, which can extract\n",
    "        the features in a given picture.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert cnn_type in [\"resnet\",\"googlenet\",\"vggnet\"]\n",
    "        self.cnn_type = cnn_type\n",
    "\n",
    "        # Here we want the output from the last convolutional layer(excluding the last two fully connected layer)\n",
    "        # The outputs are the features of the given images\n",
    "        model = None\n",
    "        if cnn_type == \"resnet\":\n",
    "            model = models.resnet50(pretrained=True)\n",
    "            for parameter in model.parameters():\n",
    "                parameter.requires_grad_(False)\n",
    "            modules = list(model.children())[:-2]\n",
    "            self.cnn_model = nn.Sequential(*modules)\n",
    "\n",
    "        elif cnn_type == \"googlenet\":\n",
    "            model = models.googlenet(pretrained=True)\n",
    "            for parameter in model.parameters():\n",
    "                parameter.requires_grad_(False)\n",
    "\n",
    "            modules = list(model.children())[:-1]\n",
    "            self.cnn_model = nn.Sequential(*modules)\n",
    "\n",
    "        elif cnn_type == \"vggnet\":\n",
    "            model = models.vgg19(pretrained=True)\n",
    "            for parameter in model.parameters():\n",
    "                parameter.requires_grad_(False)\n",
    "\n",
    "            modules = list(model.children())[:-2]\n",
    "            self.cnn_model = nn.Sequential(*modules)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,images):\n",
    "        \"\"\"\n",
    "        This function takes an pre-processed image as input and produce an encoded image output from CNN for RNN input.\n",
    "        :param input images: image_pixel [batch_size,3,224,224]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.cnn_type==\"resnet\":\n",
    "            output_features = self.cnn_model(images) #[batch_size,2048,7,7]\n",
    "\n",
    "            output_features = output_features.permute(0,2,3,1) #[batch_size,7,7,2048]\n",
    "\n",
    "            # Flatten the height and width of the image\n",
    "            output_features = output_features.view(output_features.size(0),-1,output_features.size(-1)) #[batch_size,49,2048]\n",
    "        elif self.cnn_type==\"googlenet\":\n",
    "            output_features = self.cnn_model(images)\n",
    "            output_features = output_features.permute(0, 2, 3, 1)  # [batch_size,7,7,2048]\n",
    "\n",
    "            # Flatten the height and width of the image\n",
    "            output_features = output_features.view(output_features.size(0), -1,\n",
    "                                                   output_features.size(-1))  # [batch_size,49,2048]\n",
    "\n",
    "        elif self.cnn_type==\"vggnet\":\n",
    "            output_features = self.cnn_model(images)\n",
    "            output_features = output_features.permute(0, 2, 3, 1)  # [batch_size,7,7,2048]\n",
    "\n",
    "            # Flatten the height and width of the image\n",
    "            output_features = output_features.view(output_features.size(0), -1,\n",
    "                                                   output_features.size(-1))  # [batch_size,49,2048]\n",
    "\n",
    "        return output_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attention机制\n",
    "#Implement the attention techniques\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# For testing\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    This class established the Attention Mechanism in the encoded image, also used for visualization.\n",
    "    \"\"\"\n",
    "    def __init__(self,encoder_dim,decoder_dim,attention_dim=None,attention_method=\"concat\",attention_type=\"global\"):\n",
    "        super().__init__()\n",
    "        assert attention_type in [\"global\",\"local\"]\n",
    "        assert attention_method in [\"dot\",\"general\",\"concat\"], \"method error\"\n",
    "\n",
    "        self.method = attention_method\n",
    "        self.type = attention_type\n",
    "        self.attention_dim = attention_dim\n",
    "\n",
    "        if self.type == \"local\":\n",
    "            self.attention_dim = attention_dim\n",
    "            self.Wa = nn.Linear(decoder_dim,attention_dim)\n",
    "            self.Va = nn.Linear(encoder_dim,attention_dim)\n",
    "            self.out = nn.Linear(attention_dim,1)\n",
    "\n",
    "        else:\n",
    "            if self.method == \"general\":\n",
    "                self.Wa = nn.Linear(encoder_dim,decoder_dim,bias=False)\n",
    "            elif self.method == \"concat\":\n",
    "                self.Wa = nn.Linear(encoder_dim+decoder_dim,decoder_dim,bias=False)\n",
    "                self.Va = nn.Linear(decoder_dim,1)\n",
    "\n",
    "\n",
    "    def forward(self,hidden_state,encoder_outputs):\n",
    "        \"\"\"\n",
    "        This function computes the attention_weights used for decoder RNN caption generation.\n",
    "        :param hidden_state: input hidden_state from the last LSTM cell, [num_layer,batch_size,decoder_dim]\n",
    "        :param encoder_outputs: outputs of the CNN encoder. [batch_size,seq_len,encoder_hidden_size]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.method == \"dot\":\n",
    "            return self.dot_score(hidden_state,encoder_outputs)\n",
    "\n",
    "        elif self.method == \"general\":\n",
    "            return self.general_score(hidden_state,encoder_outputs)\n",
    "\n",
    "        elif self.method == \"concat\":\n",
    "            return self.concat_score(hidden_state,encoder_outputs)\n",
    "\n",
    "\n",
    "    def dot_score(self,hidden_state,encoder_outputs):\n",
    "        \"\"\"\n",
    "        Depreciated, won't be used for the project\n",
    "        :param hidden_state: [batch_size,decoder_dim]\n",
    "        :param encoder_outputs: [batch_size,seq_len,encoder_dim]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        hidden_state = hidden_state.permute(1, 2, 0)  # [batch_size,decoder_dim,1]\n",
    "        attention_weight = encoder_outputs.bmm(hidden_state).squeeze(-1)  # [batch_size,seq_len]\n",
    "        attention_weight = F.softmax(attention_weight)\n",
    "        context = encoder_outputs * attention_weight.unsqueeze(2)\n",
    "        context = context.sum(dim=1)  # [batch_size,encoder_dim]\n",
    "        return attention_weight,context\n",
    "\n",
    "\n",
    "    def general_score(self,hidden_state,encoder_outputs):\n",
    "        \"\"\"\n",
    "        Depreciated, won't be used for the project.\n",
    "        :param hidden_state: [batch_size,decoder_dim]\n",
    "        :param encoder_outputs: [batch_size,seq_len,encoder_dim]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        encoder_seq_len = encoder_outputs.size(1)\n",
    "        print(encoder_outputs.size())\n",
    "        print(encoder_outputs.view(batch_size*encoder_seq_len,-1))\n",
    "\n",
    "        encoder_outputs = self.Wa(encoder_outputs.view(batch_size*encoder_seq_len,-1))  # [batch_size*seq_len,decoder_dim]\n",
    "        encoder_outputs = encoder_outputs.view(batch_size,encoder_seq_len,-1)  # [batch_size,seq_len,decoder_dim]\n",
    "        hidden_state = hidden_state.permute(1, 2, 0)  # [batch_size,decoder_dim,1]\n",
    "        attention_weight = encoder_outputs.bmm(hidden_state).squeeze(-1)  # [batch_size,seq_len]\n",
    "        attention_weight = F.softmax(attention_weight)\n",
    "        context = encoder_outputs * attention_weight.unsqueeze(2)\n",
    "        context = context.sum(dim=1)  # [batch_size,encoder_dim]\n",
    "        return attention_weight,context\n",
    "\n",
    "\n",
    "    def concat_score(self,hidden_state,encoder_outputs):\n",
    "        \"\"\"\n",
    "        Defining the alignment function mentioned by Luong et al.\n",
    "        :param hidden_state: [batch_size,decoder_dim]\n",
    "        :param encoder_outputs: [batch_size,seq_len,encoder_dim]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # If we use the local attention\n",
    "        if self.type==\"local\":\n",
    "            encoder_out = self.Va(encoder_outputs) #[batch_size,seq_len,attention_dim]\n",
    "            decoder_out = self.Wa(hidden_state) #[batch_size,attention_dim]\n",
    "\n",
    "            combined_states = torch.tanh(encoder_out+decoder_out.unsqueeze(1))\n",
    "\n",
    "            attention_scores = self.out(combined_states)\n",
    "\n",
    "            attention_scores = attention_scores.squeeze(2)\n",
    "\n",
    "            attention_weight = F.softmax(attention_scores,dim=1)\n",
    "\n",
    "            context = encoder_out * attention_weight.unsqueeze(2)\n",
    "\n",
    "            context = context.sum(dim=1)\n",
    "        # If we use the global attention\n",
    "        else:\n",
    "            hidden_state = hidden_state.unsqueeze(1)\n",
    "            hidden_state = hidden_state.repeat(1, encoder_outputs.size(1), 1)  # [batch_size,seq_len,decoder_dim]\n",
    "\n",
    "            concated = torch.cat([hidden_state, encoder_outputs],\n",
    "                                 dim=-1)  # [batch_size,seq_len,decoder_dim+encoder_dim]\n",
    "\n",
    "            batch_size = encoder_outputs.size(0)\n",
    "            encoder_seq_len = encoder_outputs.size(1)\n",
    "\n",
    "            attention_weight = self.Va(torch.tanh(self.Wa(concated.view(batch_size*encoder_seq_len,-1)))).squeeze(-1)  # [batch_size*seq_len]\n",
    "            attention_weight = attention_weight.view(batch_size,encoder_seq_len)\n",
    "\n",
    "            attention_weight = F.softmax(attention_weight,dim=1)  # [batch_size,seq_len]\n",
    "            context = encoder_outputs * attention_weight.unsqueeze(2) #[batch_size,seq_len,encoder_dim]\n",
    "            context = context.sum(dim=1) #[batch_size,encoder_dim]\n",
    "        return attention_weight,context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DecoderRNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyDecoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    This class Establish an attention-based Rnn Module for caption generation\n",
    "    \"\"\"\n",
    "    def __init__(self,vocab_dim,\n",
    "                 embedding_dim,\n",
    "                 encoder_dim,\n",
    "                 decoder_dim,\n",
    "                 n_layers = 1,\n",
    "                 dropout_p=0.5,\n",
    "                 attention_dim=None,\n",
    "                 attention_type = \"global\",\n",
    "                 GRU = False):\n",
    "        super().__init__()\n",
    "\n",
    "        # 定义参数\n",
    "        self.attention_type = attention_type\n",
    "        self.vocab_dim = vocab_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.GRU = GRU\n",
    "\n",
    "\n",
    "        # 定义embed层，将输入的caption 经过embed处理之后用于decoder生成image captions\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=VOCAB_DIM,\n",
    "                                            embedding_dim=embedding_dim,\n",
    "                                            padding_idx=PAD_IDX)\n",
    "\n",
    "        # 定义attention层，产生attention_score 用于后续分配注意力\n",
    "        if self.attention_type == \"global\":\n",
    "            self.attn = Attention(encoder_dim,decoder_dim)\n",
    "\n",
    "        elif self.attention_type == \"local\":\n",
    "            self.attn = Attention(encoder_dim,decoder_dim,attention_dim=self.attention_dim,attention_type=\"local\")\n",
    "\n",
    "        # 定义正则化方法\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "        # 定义初始化层\n",
    "        self.init_hidden = nn.Linear(encoder_dim,decoder_dim)\n",
    "        self.init_cell = nn.Linear(encoder_dim,decoder_dim)\n",
    "\n",
    "        # 定义每一个LSTM cell单元用于手动迭代\n",
    "        if self.attention_type==\"local\":\n",
    "            self.lstm_cell = nn.LSTMCell(embedding_dim+attention_dim,decoder_dim,bias=True)\n",
    "        else:\n",
    "            self.lstm_cell = nn.LSTMCell(embedding_dim+encoder_dim,decoder_dim,bias=True)\n",
    "\n",
    "\n",
    "        if self.GRU:\n",
    "            if self.attention_type == \"local\":\n",
    "                self.gru_cell = nn.GRUCell(embedding_dim+attention_dim,decoder_dim,bias=True)\n",
    "            else:\n",
    "                self.gru_cell = nn.GRUCell(embedding_dim+encoder_dim,decoder_dim,bias=True)\n",
    "\n",
    "\n",
    "        self.fcn = nn.Linear(decoder_dim,vocab_dim)\n",
    "\n",
    "\n",
    "    def forward(self,image_features,captions):\n",
    "        \"\"\"\n",
    "\n",
    "        :param image_features: encoder_outputs [batch_size,seq_len,encoder_dim]\n",
    "        :param captions: numericalized captions list  [batch_size,max_len]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        embedded_captions = self.embedding_layer(captions) #[batch_size,embed_dim]\n",
    "\n",
    "        # 初始化LSTM层\n",
    "        # 对所有的features取平均用于初始化hidden_state和cell_state\n",
    "        image_features_init = image_features.mean(dim=1)\n",
    "\n",
    "\n",
    "        hidden_state = self.init_hidden(image_features_init)\n",
    "        cell = self.init_cell(image_features_init)\n",
    "\n",
    "        # 遍历所有时间步\n",
    "        seq_len = len(captions[0])-1\n",
    "        batch_size = captions.size(0)\n",
    "        encoder_dim = image_features.size(1)\n",
    "\n",
    "        # 初始化一个batch_size的所有的结果\n",
    "        outputs = torch.zeros(batch_size,seq_len,self.vocab_dim).to(DEVICE)\n",
    "        attention_weights = torch.zeros(batch_size,seq_len,encoder_dim).to(DEVICE)\n",
    "\n",
    "        if self.GRU:\n",
    "            for t in range(seq_len):\n",
    "                attention_weight, context = self.attn(hidden_state, image_features)\n",
    "\n",
    "                gru_input = torch.cat([embedded_captions[:, t], context], dim=1)\n",
    "\n",
    "                hidden_state = self.gru_cell(gru_input, hidden_state)\n",
    "\n",
    "                output = self.fcn(self.dropout(hidden_state))\n",
    "\n",
    "                # 预测的词向量, output [batch_size,vocab_dim] ,attention_weight [batch_size,seq_len]\n",
    "                outputs[:, t] = output\n",
    "                attention_weights[:, t] = attention_weight\n",
    "\n",
    "        else:\n",
    "        #对于每一个lstm cell 我们都需要输入四个数据，hidden_state,cell,上一次 attention产生的context, 以及上一次的output(embedded之后的)\n",
    "            for t in range(seq_len):\n",
    "\n",
    "                attention_weight,context = self.attn(hidden_state,image_features)\n",
    "                lstm_input = torch.cat([embedded_captions[:,t],context],dim=1)\n",
    "                hidden_state, cell = self.lstm_cell(lstm_input,(hidden_state,cell))\n",
    "\n",
    "                output = self.fcn(self.dropout(hidden_state))\n",
    "\n",
    "                #预测的词向量, output [batch_size,vocab_dim] ,attention_weight [batch_size,seq_len]\n",
    "                outputs[:,t] = output\n",
    "                attention_weights[:,t] = attention_weight\n",
    "\n",
    "        return outputs,attention_weights\n",
    "\n",
    "\n",
    "    def generate_caption(self,image_features,max_len=15,vocabulary=dataset.word_dictionary):\n",
    "\n",
    "        batch_size = image_features.size(0)\n",
    "\n",
    "        image_features_init = image_features.mean(dim=1)\n",
    "        hidden_state = self.init_hidden(image_features_init)\n",
    "        cell = self.init_cell(image_features_init)\n",
    "\n",
    "\n",
    "        # Starting to feed words into the RNN decoder by <SOS>\n",
    "        word = torch.tensor(vocabulary.tokens_to_index[\"<SOS>\"]).view(1,-1).to(DEVICE)\n",
    "\n",
    "        #经过embed处理\n",
    "        embedded = self.embedding_layer(word)\n",
    "\n",
    "        attention_weights_list = []\n",
    "        caption_outputs = []\n",
    "\n",
    "        #达到最大句子长度限制就停止预测\n",
    "        if self.GRU:\n",
    "            for i in range(max_len):\n",
    "                attention_weights, context = self.attn(hidden_state, image_features)\n",
    "\n",
    "                # store the attention weights into the list\n",
    "                attention_weights_list.append(attention_weights.cpu().detach().numpy())\n",
    "\n",
    "                gru_input = torch.cat([embedded[:, 0], context], dim=1)\n",
    "\n",
    "                hidden_state = self.gru_cell(gru_input, hidden_state)\n",
    "\n",
    "                # Get a list with the likelihood of each word\n",
    "                output = self.fcn(self.dropout(hidden_state))  # [batch_size,vocab_dim]\n",
    "\n",
    "                predicted_word_index = output.argmax(dim=1)  # [batch_size,1]\n",
    "\n",
    "                caption_outputs.append(predicted_word_index.item())\n",
    "\n",
    "                # 遇到<EOS>就停止预测\n",
    "                if dataset.word_dictionary.index_to_tokens[predicted_word_index.item()] == \"<EOS>\":\n",
    "                    break\n",
    "\n",
    "                # for the next iteration\n",
    "                embedded = self.embedding_layer(predicted_word_index.unsqueeze(0))\n",
    "        else:\n",
    "            for i in range(max_len):\n",
    "                attention_weights,context = self.attn(hidden_state,image_features)\n",
    "\n",
    "                #store the attention weights into the list\n",
    "                attention_weights_list.append(attention_weights.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "                lstm_input = torch.cat([embedded[:,0],context],dim=1)\n",
    "\n",
    "\n",
    "                hidden_state,cell = self.lstm_cell(lstm_input,(hidden_state,cell))\n",
    "                #hidden_state [batch_size,decoder_dim]\n",
    "                #cell [batch_size,decoder_dim]\n",
    "\n",
    "\n",
    "                # Get a list with the likelihood of each word\n",
    "                output = self.fcn(self.dropout(hidden_state)) #[batch_size,vocab_dim]\n",
    "\n",
    "                predicted_word_index = output.argmax(dim=1) #[batch_size,1]\n",
    "\n",
    "                caption_outputs.append(predicted_word_index.item())\n",
    "\n",
    "                # 遇到<EOS>就停止预测\n",
    "                if dataset.word_dictionary.index_to_tokens[predicted_word_index.item()] == \"<EOS>\":\n",
    "                    break\n",
    "\n",
    "                # for the next iteration\n",
    "                embedded = self.embedding_layer(predicted_word_index.unsqueeze(0))\n",
    "\n",
    "\n",
    "        caption_outputs = [dataset.word_dictionary.index_to_tokens[index] for index in caption_outputs]\n",
    "\n",
    "\n",
    "        return caption_outputs,attention_weights_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    \"\"\"\n",
    "    This class encapsulates the final encoder-decoder model.\n",
    "    \"\"\"\n",
    "    def __init__(self,cnn_type=\"resnet\",attention_dim=None,attention_type=\"global\",GRU=False):\n",
    "        super().__init__()\n",
    "        self.encoder = MyEncoderCNN(cnn_type=cnn_type)\n",
    "        self.decoder = MyDecoderRNN(\n",
    "            vocab_dim=VOCAB_DIM,\n",
    "            embedding_dim=EMBED_DIM,\n",
    "            encoder_dim= ENCODER_DIM,\n",
    "            decoder_dim= DECODER_DIM,\n",
    "            attention_dim=attention_dim,\n",
    "            attention_type=attention_type,\n",
    "            GRU=GRU\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self,imgs,captions):\n",
    "        image_features = self.encoder(imgs)\n",
    "        outputs,attention_weights = self.decoder(image_features,captions)\n",
    "        return outputs,attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list_ResNetGlobalLSTM = []\n",
    "loss_list_ResNetGlobalGRU = []\n",
    "loss_list_VGG19GlobalLSTM = []\n",
    "loss_list_VGG19GlobalGRU = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam,RMSprop,SGD,ASGD,Adagrad\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def save_model(model,epochs,model_name):\n",
    "    \"\"\"\n",
    "    This function saves the model's parameters\n",
    "    :param model:\n",
    "    :param epoch:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    model_param = {\n",
    "        \"epochs\":epochs,\n",
    "        \"vocab_size\":VOCAB_DIM,\n",
    "        \"embed_size\":EMBED_DIM,\n",
    "        \"encoder_dim\":ENCODER_DIM,\n",
    "        \"decoder_dim\":DECODER_DIM,\n",
    "        \"attention_dim\":ATTENTION_DIM,\n",
    "        \"model_state_dict\":model.state_dict()\n",
    "    }\n",
    "\n",
    "    torch.save(model_param,\"./模型存放/{}.pkl\".format(model_name))\n",
    "\n",
    "\n",
    "def evaluation(epoch,model,model_name,loss_list,display_steps=20):\n",
    "    \"\"\"\n",
    "    This function serves to dynamically evaluate the model's performance.\n",
    "    :param epoch:\n",
    "    :param display_steps:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    for idx, (image, targets) in enumerate(get_data_loader(train=True)):\n",
    "        # image [batch_size,seq_len,encoder_dim]\n",
    "        # targets [batch_size,max_len]\n",
    "\n",
    "        image, targets = image.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # outputs [batch_size,seq_len,vocab_size]\n",
    "        outputs, attention_weights = model(image, targets)\n",
    "\n",
    "        target = targets[:, 1:]  # 取<SOS>之后的文本序列,target [batch_size,seq_len]\n",
    "\n",
    "        # 计算一个batch上的交叉损失，用于backpropagation\n",
    "        l = criterion_metrics(outputs.view(-1, VOCAB_DIM), target.reshape(-1))\n",
    "\n",
    "        l.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if (idx + 1) % (display_steps) == 0:\n",
    "            print(\"Epoch: {} loss: {:.5f}\".format(epoch + 1, l.item()))\n",
    "            loss_list.append(l.item())\n",
    "            # 切换成评估模式(忽略dropout)\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                data_loader = iter(get_data_loader(train=False))\n",
    "                image, caption = next(data_loader)\n",
    "                image_features = model.encoder(image[0:1].to(DEVICE))\n",
    "\n",
    "                captions_result, attention_weights = model.decoder \\\n",
    "                    .generate_caption(image_features)\n",
    "\n",
    "                sentence = \" \".join(captions_result)\n",
    "\n",
    "                show_image(image[0], label=sentence)\n",
    "            model.train()\n",
    "        save_model(model,epoch,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResNetGlobalLSTM\n",
    "seq2seq = MyModel().to(DEVICE)\n",
    "\n",
    "\n",
    "optimizer = Adam(seq2seq.parameters(),lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "criterion_metrics = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(2):\n",
    "    evaluation(i,\"ResNetGlobalLSTM\",loss_list_ResNetGlobalLSTM,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
