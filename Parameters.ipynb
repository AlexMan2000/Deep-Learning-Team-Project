{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-bb725b4001e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = pd.read_csv(r\"F:\\Downloads\\glove.6B.200d.txt\",sep=\" \", quoting=3, header=None, index_col=0)\n",
    "glove_embedding = {key: val.values for key, val in glove.T.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(word_index,embedding_dict,dimension):\n",
    "    embedding_matrix=np.zeros((len(word_index)+1,dimension))\n",
    "\n",
    "    for word,index in word_index.items():\n",
    "        if word in embedding_dict:\n",
    "            if word not in [\"<UNK>\",\"<PAD>\",\"SOS\",\"EOS\"]:\n",
    "                embedding_matrix[index]=[*embedding_dict[word],0,0,0,0,0]\n",
    "            else:\n",
    "                embedding_matrix[index] = embedding_dict[word]\n",
    "    return embedding_matrix\n",
    "original_embedding_len = len(glove_embedding[\"<unk>\"])\n",
    "glove_embedding[\"<UNK>\"] = [*glove_embedding[\"<unk>\"],0,1,0,0,0]\n",
    "glove_embedding[\"<PAD>\"] = [original_embedding_len*0,0,0,0,1]\n",
    "glove_embedding[\"<SOS>\"] = [original_embedding_len*0,0,0,1,0]\n",
    "glove_embedding[\"<EOS>\"] = [original_embedding_len*0,0,1,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVocab:\n",
    "    \"\"\"\n",
    "        This class is responsible for constructing the dictionary which contains\n",
    "        all the words that appear over a certain frequency, which we will use to\n",
    "        tokenize any given sentence for our RNN model.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        #Pre restore the tokens mapping.\n",
    "        \"\"\"\n",
    "        These are severals pre-defined tokens to pre process the sentence(sequqnce).\n",
    "        <PAD>: Used to pad any given sentence to a uniform length, making it easier for\n",
    "        RNN model to handle.\n",
    "        <SOS>: Inserted at the start of each sentence.\n",
    "        <EOS>: Appended at the end of each sentence.\n",
    "        <UNK>: Mark the word that hasn't appeared in the captions in the training data.\n",
    "        \"\"\"\n",
    "        self.index_to_tokens = {0:\"<PAD>\",1:\"<SOS>\",2:\"<EOS>\",3:\"<UNK>\"}\n",
    "\n",
    "        #Inverse the above dictionary\n",
    "        self.tokens_to_index = {value:key for key,value in self.index_to_tokens.items()}\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        :return: int The number of the stored tokens\n",
    "        \"\"\"\n",
    "        return len(self.index_to_tokens)\n",
    "\n",
    "\n",
    "    def build_vocab(self,sentence_list,min_count=1,max_count=None,max_features=None):\n",
    "        \"\"\"\n",
    "        This function builds the dictionary for RNN model\n",
    "        :param sentence_list: An iterable containers that includes all the sentences\n",
    "        :param min_count: The minimum number of the time that a word should appear in all the sentences.\n",
    "        :param max_count: The maximum number of the time that a word should appear in all the sentences.\n",
    "        :param max_features: Number of words to keep(From the most frequent words).\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        #Create a dictionary for counting word frequency\n",
    "        self.frequency_counter = {}\n",
    "\n",
    "        #Create word_dict from several sentences\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenize(sentence):\n",
    "                self.frequency_counter[word] = self.frequency_counter.get(word,0)+1\n",
    "\n",
    "        #Filtering\n",
    "        if min_count is not None:\n",
    "            self.frequency_counter = {word:value for word,value in self.frequency_counter.items() if value >= min_count}\n",
    "\n",
    "        if max_count is not None:\n",
    "            self.frequency_counter = {word:value for word,value in self.frequency_counter.items() if value <= max_count}\n",
    "\n",
    "        if max_features is not None:\n",
    "            self.frequency_counter = dict(list(sorted(self.frequency_counter.items(),key=lambda x:x[-1],reverse=True))[:max_features])\n",
    "\n",
    "        #Creating words_to_index mapping\n",
    "        for word in self.frequency_counter:\n",
    "            self.tokens_to_index[word] = len(self.tokens_to_index)\n",
    "\n",
    "\n",
    "        #Creating index_to_words mapping\n",
    "        self.index_to_tokens = dict(zip(self.tokens_to_index.values(),self.tokens_to_index.keys()))\n",
    "\n",
    "\n",
    "\n",
    "    def sentence_to_index(self,sentence,max_len = 20):\n",
    "        \"\"\"\n",
    "        This function converts the sentence to word index and controls\n",
    "        the maximum length of the sentence. Meanwhile, it adds <SOS> and <EOS> tags\n",
    "        to the beginning and the ending of a given sentence.\n",
    "        :param sentence: string, A sentence in string.\n",
    "        :param max_len: int, performing sentence pruning\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        tokenized_sentence = self.tokenize(sentence)\n",
    "        if max_len is not None:\n",
    "            tokenized_sentence = tokenized_sentence[:max_len]\n",
    "\n",
    "        return [self.tokens_to_index.get(word,self.tokens_to_index[\"<UNK>\"]) for word in tokenized_sentence]\n",
    "\n",
    "\n",
    "    def index_to_sentence(self,indices):\n",
    "        \"\"\"\n",
    "        This function converts the index back to caption words, used for visualization.\n",
    "        :param indices: A list of the index of words, e.g. [2,3,6,9,10,...]\n",
    "        :return: A list of word corresponding to the indices, [\"today\",\"good\",\"date\",...]\n",
    "        \"\"\"\n",
    "        return [self.index_to_tokens.get(index) for index in indices]\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(content):\n",
    "        tokens = [token.text.lower() for token in SPACY_OBJ.tokenizer(content)]\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=[\"The cat sat on mat\",\"we can play with model\"]\n",
    "\n",
    "word_dict = MyVocab()\n",
    "word_dict.build_vocab(text)\n",
    "print(word_dict.tokens_to_index)\n",
    "\n",
    "embedding_matrix=create_embedding_matrix(word_dict.tokens_to_index,embedding_dict=glove_embedding,dimension=100)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.1113e-01, -4.7518e-01,  2.2871e-01,  8.6524e-03, -4.3737e-01,\n",
       "       -2.8747e-01,  2.3416e-01, -2.0332e-02,  5.0697e-01, -2.4367e-01,\n",
       "       -2.8646e-01, -2.4133e-02, -9.6845e-06,  4.8092e-02, -2.4467e-01,\n",
       "       -1.2121e-01,  1.3644e-01, -1.6190e-01,  9.9349e-02,  3.6545e-02,\n",
       "       -3.1657e-02, -8.4172e-01,  2.3022e-01,  3.0332e-02, -6.7638e-01,\n",
       "       -2.9399e-01,  1.3298e-01, -3.7917e-02, -1.0970e-01,  2.6541e-03,\n",
       "       -4.6669e-01,  1.2329e-01, -4.0373e-03, -2.8782e-01, -1.6733e-01,\n",
       "        2.0938e-01,  4.9163e-01, -6.0818e-02,  1.4326e-01, -1.3628e-01,\n",
       "       -1.5650e-01, -8.3060e-02, -8.5820e-02, -1.9864e-01,  3.6016e-01,\n",
       "       -1.6752e-01, -6.4389e-03,  2.3173e-01, -1.6636e-01, -1.7120e-01,\n",
       "        1.8400e-01, -5.2889e-01, -7.0440e-02, -3.7772e-01, -6.6473e-02,\n",
       "        3.8519e-01,  2.6262e-01, -3.9148e-02, -2.8182e-01, -1.6908e-01,\n",
       "       -4.9491e-01, -1.4138e-01,  4.1732e-01, -6.8889e-02,  1.0125e-01,\n",
       "        1.8499e-02, -6.6256e-02,  1.9328e-01, -2.4784e-01,  2.5005e-01,\n",
       "       -6.3437e-02,  2.6013e-01,  2.3620e-01, -2.9019e-01,  4.9803e-01,\n",
       "       -7.4534e-01,  2.7620e-01,  1.6096e-01, -2.4558e-01,  1.6969e-01,\n",
       "       -1.1854e-01,  7.3353e-01,  1.3268e-01, -6.3005e-02,  1.2337e-01,\n",
       "        6.2225e-02,  7.8222e-02,  1.3446e-01, -6.0425e-01,  1.5230e-01,\n",
       "        2.8806e-01,  1.8001e-01, -2.8224e-01,  2.7971e-01,  1.1618e-01,\n",
       "       -1.9708e-02, -9.8518e-03, -7.0856e-02,  1.7582e-01,  1.2370e-01,\n",
       "       -1.9066e-01, -3.0569e-02,  3.3535e-01,  5.1755e-02,  6.4251e-02,\n",
       "        8.1222e-02,  9.3289e-02, -5.5357e-01,  1.5639e-02, -4.5546e-01,\n",
       "        2.7916e-01,  5.0138e-01, -3.0025e-01, -2.1612e-01, -6.2151e-02,\n",
       "       -1.6650e-01, -1.5149e-01, -2.3214e-01,  1.8798e-01,  2.8888e-01,\n",
       "       -3.6444e-01, -4.4035e-01,  1.1130e-03, -1.8800e-01, -2.6090e-01,\n",
       "        2.5960e-01, -5.1463e-02, -4.3329e-02,  1.9906e-01, -1.5509e-01,\n",
       "        5.9359e-01, -2.1597e-01, -3.9197e-01,  1.7550e-01, -9.3492e-02,\n",
       "       -7.0023e-02,  8.1829e-02,  3.2887e-01,  1.0574e-01, -2.6531e-02,\n",
       "       -1.3022e-01,  2.6618e-01, -2.0827e-01,  1.5629e-01, -6.2249e-01,\n",
       "       -3.2305e-01, -3.2544e-01,  8.2363e-02,  8.5395e-02,  2.0349e-01,\n",
       "        7.0146e-01, -5.4315e-01,  4.3537e-01, -9.8362e-02, -2.0338e-01,\n",
       "        1.2440e-01,  3.4278e-01, -2.3244e-03, -2.4317e-02,  5.3333e-01,\n",
       "       -6.7459e-02,  1.6801e-01, -1.9277e-01,  2.4257e-01,  2.5454e-02,\n",
       "       -7.8018e-02, -9.8558e-02, -3.8660e-01,  4.5259e-01, -3.3174e-01,\n",
       "        2.8998e-01,  2.2814e-01, -7.0167e-02, -3.0188e-01,  4.6792e-01,\n",
       "       -3.6618e-02,  2.3412e-01,  4.1025e-01, -2.4629e-02,  1.8839e-01,\n",
       "       -4.1109e-01,  3.9102e-01,  1.1888e-01, -1.2482e-02, -2.1707e-01,\n",
       "       -8.0927e-02,  2.0984e-01, -5.9146e-01,  1.6199e-01, -4.3054e-02,\n",
       "        3.1073e-02,  1.2325e-01,  1.2524e-01,  7.7474e-02,  4.3502e-01,\n",
       "        7.1070e-02,  1.8514e-01, -5.7218e-03,  1.6466e-01, -3.9074e-01])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
